<html>
  <head>
    <title>Aplicaciones Multimedia: Práctica GStreamer (III)</title>
    <meta http-equiv="content-type" content="text/html; charset=iso-8859-1" />
    <link rel="SHORTCUT ICON" href="http://www.uc3m.es/favicon.ico" /> 
    <link href="http://www.it.uc3m.es/rcrespo/docencia/asignatura.css" rel="stylesheet" type="text/css" />
    <link href="http://www.it.uc3m.es/estilo.css" rel="stylesheet" type="text/css" />
    <link href="http://www.it.uc3m.es/rcrespo/docencia/amm/1112/p3/css/amm.css" rel="stylesheet" type="text/css" />
    <link href="../css/classon.css" rel="stylesheet" type="text/css" />
    <link href="../css/jquery-ui.css" rel="stylesheet" type="text/css" />
    
    <script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
    <script type="text/javascript" src="../js/jquery.blockUI.js"></script>
    <style type="text/css">
      .code {
        color: blue;
        font-size: 80%;
      }
      .metadatos {
        color: green;
        font-style: italic;
      }
    </style>
  </head>
  <body>
    
    <!-- BEGIN ADD, 21/05/09; to comply with UC3M corporate image requirements -->
    <div class="headerBorderUc3mblue"></div>
    <div class="header white">
      <a href="http://www.uc3m.es/">
        <img class="logos logoleft" src="http://www.it.uc3m.es/imag/EscudoLogoCorporativo.png" alt="Universidad Carlos III de Madrid" />
      </a>
        <a href="http://www.it.uc3m.es">
          <img class="logos logoright" src="http://www.it.uc3m.es/imag/SpanishLogoIT.png" alt="Departamento de Ingeniería Telemática" />
        </a>
    </div>
    <div class="headerBorderUc3mblue"></div>
    <!-- END ADD 21/05/09 -->
    
    
    <!-- Tabla encabezado departamento -->
    <table width="100%" border="0" cellspacing="0" cellpadding="0" bgcolor="#000000">
      <tr>
        <td>
          <table width="100%" border="0" cellspacing="1" cellpadding="0">    
            <tr>
              <td bgcolor="#CCD0D6" width="2000" height="22">
                <div align="right"><a href="http://www.it.uc3m.es/localizacion/localizacion.htm" class="azul">Localizaci&oacute;n</a>
                  | <a href="#"></a><a href="http://www.it.uc3m.es/personal/directorio.htm"><span class="rojo">Personal</span></a>
                  | <a href="http://www.it.uc3m.es/docencia/docencia.htm" class="azul">Docencia</a>
                  | <a href="http://www.it.uc3m.es/investigacion/investigacion.htm" class="azul">Investigaci&oacute;n</a>
                  | <a href="http://www.it.uc3m.es/novedades/novedades.htm" class="azul">Novedades</a>
                  | <a href="http://www.it.uc3m.es/intranet/intranet.htm" class="azul">Intranet</a>
                  &nbsp;&nbsp;</div>
              </td>
            </tr>
          </table>
        </td>
      </tr>
    </table>
    <!-- Tabla encabezado departamento (fin) -->
    
    <!-- Navegación -->
    <nav>
      <p class="nav"><a class="azul"
      href="http://www.it.uc3m.es/index.html">Home</a> / <a class="azul"
      href="http://www.it.uc3m.es/docencia/docencia.htm">Docencia</a> / <a
      class="azul" href="http://www.it.uc3m.es/docencia/grado-audiovisuales.html">Grado en Ingeniería de Sistemas Audiovisuales</a> / 
      <a class="azul" href="http://www.it.uc3m.es/rcrespo/docencia/amm/1011/">Aplicaciones Multimedia</a>
      </p>
    </nav>
    <!-- Navegación (fin) -->
    
    <header>
      <h1>Pr&aacute;ctica 8: Programación con GStreamer</h1>
    </header>
    <section id="intro">
      <h2>Introducci&oacute;n</h2>
      <!-- <h3>Fundamentos tecnológicos</h3> -->

<!--
      <p>
	GStreamer es un entorno para creación de aplicaciones de
	streaming multimedia.
	
	El diseño de GStreamer está basado en tuberías
	(<em>pipelines</em>), que definen el flujo de datos,
	y <em>plugins</em>, elementos que proporcionan las distintas
	funcionalidades y pueden interconectarse dentro de
	un <em>pipeline</em>.

	GStreamer permite procesar audio, vídeo y cualquier tipo de
	flujo de datos.
	
	Los formatos soportados incluyen: MP3, Ogg/Vorbis, MPEG-1/2,
	AVI, Quicktime, mod, etc.
      </p>

      <p>
	GStreamer proporciona más de 150 plugins, que pueden
	clasificarse de acuerdo a la funcionalidad que implementan:
      </p>
      <ul>
	<li>Gestión de protocolos</li>
	<li>Fuentes</li>
	<li>Formato: parsers, formateadores, muxers, demuxers, metadatos, subtítulos</li>
	<li>Codecs: codificadores y decodificadores</li>
	<li>Filtros: conversores, mixers, efectos, ...</li>
	<li>Sumideros</li>
      </ul>

      <figure>
	<img src="http://gstreamer.freedesktop.org/data/doc/gstreamer/head/manual/html/images/gstreamer-overview.png" />
        <figcaption><p><em>Figura 1</em>: Arquitectura GStreamer</p></figcaption>
      </figure>

      <p>
      </p>
-->

      <h3>Desarrollo de un programa GStreamer:</h3>
      <p>
	Para editar el código fuente, podemos utilizar cualquier editor de texto
	plano (emacs, vi, etc.).
      </p>
      <p>
	Para compilar y linkar el código fuente para generar el programa
	ejecutable, utilizaremos las herramientas de desarrollo C de GNU. En
	particular, el compilador gcc.
      </p>
      <p>
	Observa que si el código fuente está basado en GStreamer, necesitaremos
	especificar las opciones adecuadas, por ejemplo, para indicarle al
	compilador dónde encontrar las librerías y ficheros .h.  El programa
	pkg-config facilita esta tarea, en vez de tener que indicarlo
	manualmente.  El comando:
	<pre class="code">pkg-config --cflags --libs gstreamer-0.10</pre>
	genera los flags adecuados para compilar código fuente C utilizando las librerías GStreamer versión 0.10.
      </p>
      <p>
	El comando pkg-config puede integrarse directamente en la invocación al
	compilador:
      <pre class="code">gcc -Wall [program].c -o [program] $(pkg-config --cflags --libs gstreamer-0.10)</pre>
      </p>

      <h3>Objetivo</h3>
      <p>
	En esta práctica utilizaremos GStreamer para desarrollar algunas
	aplicaciones multimedia básicas.
      </p>

      <p>
 	Definiremos diversos <em>pipelines</em> interconectando
	algunos <em>plugins</em> proporcionados por el entorno, para
	implementar algunas funcionalidades básicas:
      </p>
      <ul>
        <li>Reproducción multimedia</li>
	<li>Conversión de formatos</li>
	<li>Generación de nuevos medios</li>
	<!-- <li>Captura de audio</li> -->
      </ul>
      <p>
	Una vez diseñados los pipelines, desarrollaremos los correspondientes
	programas C que implementen la misma funcionalidad haciendo uso de las
	librerías GStreamer.
      </p>

      <p>
	En concreto, extraeremos una secuencia de fotogramas de un vídeo y la
	utilizaremos para generar un nuevo montaje combinándola con el audio
	capturado del micrófono.
      </p>
    </section>



    <section id="step1">

      <h2>Paso 1: Manejo del bus</h2>
      <p>
	El bus es un sistema que permite propagar los mensajes desde los hilos
	(<em>threads</em>) del flujo multimedia al hilo (<em>thread</em> de la
	aplicación). Es decir, permite a la aplicación recibir información del
	pipeline y de los elementos del mismo.
      </p>

      <p>
	Cada pipeline tiene un bus asociado por defecto. La aplicación no tendrá
	que crearlo, simplemente registrar un manejador de mensajes, similar a
	un escuchador de señales. Una vez esté en ejecución el bucle principal,
	se comprueba periódicamente el bus y se invoca el manejador cada vez que
	hay un mensaje disponible.
      </p>

      <p>
	En
	el <a href="https://aulaglobal2.uc3m.es/mod/resource/view.php?id=944757">ejemplo
	de reproductor de audio Ogg</a> puedes ver cómo se recupera el bus del
	pipeline y se registra el manejador correspondiente. En este ejemplo, el
	manejador gestiona el mensaje de fin de flujo (<em>EOS: End Of
	Stream</em>) y mensajes de error.
      </p>

      <p>
	Descarga el código fuente, compílalo y prueba el ejecutable. Puedes
	utilizar
	el <a href="http://download.blender.org/peach/trailer/trailer_400p.ogg">trailer
	de Big Buck Bunny</a> para las pruebas. Analiza el código programado
	para gestionar los mensajes del bus, tanto para registrar el manejador
	como la función en sí.
      </p>

    </section>



    <section id="step2">
      <h2>Paso 2: Extraer fotogramas de un vídeo</h2>
      <p>
	En este paso vamos a crear un pipeline que extraiga fotogramas de un
	vídeo (formato Ogg) y los almacene en un conjunto de ficheros (formato
	PNG).  Puedes utilizar
	el <a href="http://download.blender.org/peach/trailer/trailer_400p.ogg">trailer
	de Big Buck Bunny</a> para las pruebas.
      </p>
      <ol>
      <li>
	En primer lugar, construye un pipeline básico para reproducir vídeo Ogg.
	<!-- 
	(<a href="https://aulaglobal2.uc3m.es/mod/resource/view.php?id=683924">primera
	práctica de GStreamer -paso 4-</a>). -->
      </li>
      <li>
	En lugar de visualizar el vídeo en pantalla, queremos almacenar los
	fotogramas en ficheros.  Por tanto, habrá que cambiar el sumidero: busca
	con gst-inspect un elemento sumidero que escriba la salida en un
	<em>conjunto</em> de ficheros.
	<p>
	  <em>Nota:</em> observa que el nombre por defecto para los archivos en que se
	  almacenan las imágenes sigue el patrón "%05d" (00001, 00002,
	  ...). Para especificar otro nombre, tendrás que utilizar la propiedad
	  location y definir el patrón adecuado. Por ejemplo, "frame-%05d.png"
	  (que generará los ficheros frame-00001.png, frame-00002.png, ...).
	</p>
      </li>
      <li>
	El flujo de datos en el pipeline es una señal de vídeo. Sin embargo, en
	los ficheros queremos almacenar imágenes PNG. Tendremos por tanto que
	intercalar el codificador adecuado antes del sumidero: busca con
	gst-inspect el elemento adecuado para actuar como codificador PNG.
	<p>
	  <em>Nota:</em> observa que el codificador PNG incluye una propiedad snapshot,
	  que envía una señal de fin de flujo (EOS = End Of Stream) tras
	  codificar un fotograma. Como en nuestro caso queremos almacenar varios
	  fotogramas, no debería enviarse dicha señal, por lo que esa propiedad
	  debe estar a <em>false</em>.
	</p>
	<p>
	  <em>Nota:</em> observa también que el decodificador Theora genera una señal
	  video/x-raw-yuv (espacio de colores YUV) mientras que el codificador
	  PNG trabaja con el espacio de colores RGB (como el sumidero
	  ximagesink), por lo que sigue siendo necesario un elemento que
	  convierta de un espacio de colores a otro.
	</p>
	<p>
	  Una vez diseñado y probado el pipeline, desarrolla el programa C
	  correspondiente. Incluye el código necesario para presentar en consola
	  un texto informativo cada vez que se recibe un mensaje en el bus. 
	</p>
      </li>
      </ol>

    </section>


    <section id="step3">
      <h2>Paso 3: Superponer información sobre los fotogramas</h2>
      <p>
	En cada fotograma, vamos a superponer el tiempo de grabación
	transcurrido así como la hora (reloj).  Para ello, tendremos que
	intercalar en el pipeline los elementos de procesamiento adecuados.
	<!--
	(<a href="https://aulaglobal2.uc3m.es/mod/resource/view.php?id=683924">primera
	práctica de GStreamer -paso 5-</a>).
	-->
      </p>
      <p>
	Modifica el programa anterior para agregar la funcionalidad pedida.
      </p>
    </section>

    <section id="step4">
      <h2>Paso 4: Reproducción de vídeo a partir de una secuencia de imágenes</h2>

      <p>
	En este paso vamos a visualizar un montaje con las imágenes extraídas en
	los pasos anteriores.  Para ello tendremos que construir un reproductor,
	similar a los vistos hasta ahora, sólo que usando como fuente de datos
	un elemento que lea la información de un conjunto de ficheros (en vez de
	un stream de vídeo directamente).
      </p>
      <p>
	Busca el elemento adecuado y consulta su documentación con gst-inspect.
      </p>
      <p>
	Observa que para indicar la ruta de los archivos con las imágenes
	tendrás que utilizar un patrón de nombres similar al que utilizaste para
	el sumidero (paso 1).
      </p>
      <p>
	También tendrás que especificar el formato de la señal, utilizando la
	propiedad <i>caps</i>:
      </p>
      <pre class="code">caps="image/png,framerate=\(fraction\)2/1"</pre>

      <p>
	Una vez diseñado y probado el pipeline, desarrolla el programa C
	correspondiente. Puedes basarte en los reproductores de vídeo
	programados en prácticas anteriores, introduciendo las modificaciones
	necesarias. No olvides incluir el código necesario para gestionar los
	mensajes del bus.
      </p>

    </section>


    <section id="step5">
      <h2>Paso 5: Generación de medios a partir de una secuencia de imágenes</h2>
      <p>
	En este paso, en lugar de visualizar la secuencia de imágenes como un vídeo, vamos a almacenarla como un fichero en formato Ogg. 
	Para ello habrá que:
      </p>
      <ol>
	<li>Insertar un elemento que codifique la señal de vídeo en formato
	Theora (habitualmente utilizado para la imagen en el formato Ogg)</li>
	<li>Insertar un multiplexor Ogg, puesto que dicho formato admite varios
	canales, para que genere la información necesaria de acuerdo a las
	especificaciones del formato.</li>
	<li>Utilizar como sumidero un elemento que grabe la salida a
	archivo.</li>
      </ol>

      <p>
	Modifica el programa anterior para implementar la funcionalidad pedida.
      </p>

      <p>Funcionalidad avanzada:</p>
      <ul>
	<li>
	  En lugar de programar dos reproductores distintos (para visualizar por
	  pantalla o grabar como fichero) implementa un único programa que
	  permita al usuario elegir la opción mediante línea de comandos.
	</li>
	<li>
	  Modifica el pipeline (y el programa) de modo que implemente ambas
	  funciones: visualizar por pantalla y grabar como fichero,
	  simultáneamente.
	</li>
      </ul>

    </section>


    <section id="step6">
      <h2>Paso 6: Captura de audio</h2>

      <p>
	En este paso, vamos a generar un nuevo audio para el vídeo resultante de
	la extracción de fotogramas.  Para ello, seguiremos las instrucciones
	del <em>paso 7 de
	la <a href="https://aulaglobal2.uc3m.es/mod/resource/view.php?id=942123">primera
	práctica de GStreamer</a></em>, que se reproducen a continuación.
      </p>
      <p>
	GStreamer proporciona diversas fuentes para implementar distintas
	acciones con la salida del pipeline: comprobar redundancia, envío de
	fichero, envío de la señal mediante distintos protocolos, almacenamiento
	en fichero (uno o varios), etc.
      </p>
      <p>
	En este paso, vamos a guardar la señal de audio generada en un
	fichero, en vez de reproducirla con la tarjeta de sonido.
	Utiliza gst-inspect para identificar el elemento sumidero
	adecuado, que almacene un flujo multimedia (la salida del
	pipeline) en un archivo.  Utiliza gst-inspect también para
	revisar las propiedades de dicho elemento y, en concreto, cómo
	especificar el nombre del fichero de salida.
      </p>
      <p>
	Además, vamos a capturar la entrada de audio del micrófono, en
	vez de procesar un archivo multimedia preexistente.  Para
	ello, utilizaremos la
	fuente <span class="code">alsasrc</span>, que permite
	utilizar como origen del flujo multimedia un dispositivo de
	sonido del sistema (en este caso el micrófono).
      </p>
      <p>
	<span class="code">alsasrc</span> requiere que se especifique
	el dispositivo que se utilizará como origen. Puedes utilizar
	el comando <span class="code">arecord -l</span> para listar
	los dispositivos de captura disponibles en el
	sistema. Deberías obtener una salida similar a:
      </p>
      <pre>
bash$ arecord -l
**** List of CAPTURE Hardware Devices ****
card 0: Intel [HDA Intel], device 0: STAC92xx Analog [STAC92xx Analog]
  Subdevices: 1/1
  Subdevice #0: subdevice #0
      </pre>
      <p>
	En nuestro caso, el micrófono está identificado como el
	dispositivo <span class="code">hw:0</span>.
      </p>
      <p>
	La señal del micrófono requiere algunas transformaciones antes
	de poder almacenarla en un archivo multimedia válido. En
	concreto, tendremos que convertirla al formato adecuado. En
	este caso, queremos grabar el audio en formato Ogg Vorbis.
      </p>
      <!-- Este apartado se deja más abierto puesto que el alumno ya
	   ha trabajado paso a paso con varios pipelines -->
      <p>
	<b>Importante:</b> Utiliza el conector posterior del ordenador para el
	micrófono (el delantero es posible que falle). Asegúrate también de que
	los controles no están silenciados, comprúebalo con el control
	de volumen (ver figura).
      </p>
      <figure>
	<img src="img/control_de_volumen_menu.png" />
	<img src="img/control_de_volumen.png" />
        <figcaption>Control de volumen.</p></figcaption>
      </figure>

    </section>

    <section id="step7">
      <h2>Paso 7: Procesamiento de señales multicanal</h2>

      <p>
	En este paso, vamos a combinar las señales de audio y vídeo generadas
	previamente a partir del micrófono y la secuencia de imágenes,
	respectivamente.  Para ello, seguiremos las instrucciones del <em>paso 8
	de
	la <a href="https://aulaglobal2.uc3m.es/mod/resource/view.php?id=942123">primera
	práctica de GStreamer</a></em>, que se reproducen a continuación.
      </p>

      <p>
	Al igual que puede descomponerse un flujo multimedia en
	múltiples canales, podemos también combinar múltiples señales
	en un único flujo (por ejemplo, para integrar audio y
	video). En este caso, el elemento necesario será un
	multiplexor.
      </p>

      <p>
	A continuación se muestra la estructura genérica de un
	pipeline que integra dos canales de entrada (audio y video) en
	un único flujo multimedia.
      </p>
      <pre class="code">
	gst-launch fuente_video ! transformaciones_video ! codificador_video ! queue ! multiplexor name="muxer" \
	  fuente_audio ! transformaciones_audio ! codificador_audio ! queue ! muxer. \
	  muxer. ! procesamiento_flujo_multiplexado
      </pre>

      <p>
	Observa que el nombre asignado al multiplexor es arbitrario,
	aunque obviamente se recomienda que sea descriptivo y
	significativo.

	Observa también que hemos separado en varias líneas el comando
	utilizando la barra de escape '\', pero es sólo por facilitar
	la legibilidad (no es necesario).

	Además, observa que para referirnos al multiplexor se utiliza
	el nombre asignado a dicho elemento seguido de un punto ('.'),
	con lo que puede reutilizarse en varios puntos del pipeline. 
	Verás que el multiplexor aparece tres veces en el pipeline:
      </p>
	<ul>
	  <li>la primera vez recibe como entrada la señal de vídeo</li>
	  <li>la segunda vez recibe como entrada la señal de audio</li>
	  <li>
	    la tercera vez para indicar que se utiliza su salida como
	    entrada para las siguientes etapas (presta atención que es
	    necesario repetirlo explícitamente)
	  </li>
	</ul>

      <p>
	Fíjate que también aquí se utiliza un elemento <em>queue</em> en cada
	entrada del multiplexor.  Finalmente, observa que la
	etapa <em>procesamiento_flujo_multiplexado</em> debe 
	sustituirse por la secuencia de elementos apropiada. Para
	almacenar la señal generada en un fichero, simplemente se
	utilizaría el sumidero adecuado. Si en cambio queremos
	reproducir la señal generada, tendríamos que encadenar un
	pipeline similar al construido en pasos anteriores (incluyendo
	la etapa de demultiplexación).
      </p>

      <p>
	Basándote en el esquema proporcionado, construye un pipeline que genere
	un archivo Ogg combinando las señales de audio y vídeo generada en los
	pasos anteriores.
      </p>

    </section>

    <section id="repaso">
      <h2>Repaso (práctica 2)</h2>

      <h3>Procesamiento de flujos multimedia con varios canales</h3>
      <p>
	Recuerda que cuando se procesan varios canales en paralelo en
	un <em>pipeline</em>, se necesita un elemento <em>queue</em> para cada
	uno de los flujos. El elemento queue evita que se entre en situaciones
	de bloqueo cuando se procesan varios flujos en paralelo.
      </p>

      <h3>Conexión de PADs dinámicos</h3>
      <p>
	Cuando los elementos utilizados en el pipeline proporcionan PADs
	estáticos, pueden conectarse directamente, con las
	funciones <em>gst_element_link</em> o <em>gst_element_link_many</em>.
      </p>

      <p>
	Sin embargo, algunos PADs no existen al crear el elemento, sino que se
	crean una vez se está procesando el flujo multimedia. Por ejemplo, un
	demultiplexor Ogg recibe siempre una señal Ogg, de modo que su pad sink
	existe de forma estática. Sin embargo, hasta que no recibe una señal
	real, no puede determinar si tendrá canal de audio, o de vídeo, o
	ambos. Por ello, no dispone inicialmente de pads de tipo src (para
	conectarlo al siguiente elemento). Dichos pads se crearán de forma
	dinámica una vez empiece a procesarse la señal. Por tanto, la conexión
	no podrá hacerse hasta que dichos pads se creen, de forma dinámica.
      </p>

      <p>
	La señal que alerta de que se ha creado un pad dinámico
	es <em>pad-added</em>. El programa deberá registrar una función
	escuchadora que responda cuando se produzca esta señal. Una vez se
	recibe la señal <em>pad-added</em>, significa que el pad existe, de modo
	que ya puede conectarse. La función escuchadora deberá encargarse de
	realizar dicha conexión.
      </p>

      <p>
	Para registrar una función escuchadora, de modo que se ejecute al producirse una determinada señal, se utiliza la función <em>g_signal_connect</em>:
      </p>
      <pre class="code">g_signal_connect(instance, detailed_signal, c_handler, data)</pre>
      <p>
	Siendo:
      </p>
      <ul>
	<li>instance: instancia (elemento GStreamer en este caso) que genera la
	señal</li>
	<li>detailed_signal: señal a escuchar</li>
	<li>c_handler: manejador (escuchador) de señal</li>
	<li>data: datos adicionales</li>
      </ul>

      <p>
	En el <a href="https://aulaglobal2.uc3m.es/mod/resource/view.php?id=944757">reproductor de audio Ogg</a> de ejemplo:
      </p>
      <pre class="code">g_signal_connect (demuxer, "pad-added", G_CALLBACK (on_pad_added), decoder);</pre>
      <p>
	Registra la función <em>on_pad_added</em> como escuchadora de la
	señal <em>pad-added</em> generada por el
	elemento <em>demuxer</em>. Cuando se invoque la función, recibirá además
	como datos adicionales la referencia al elemento <em>decoder</em>. En
	este caso se pasa el elemento <em>decoder</em> porque estamos trabajando
	sólo con el canal de audio.
      </p>

      <p>
	Si pueden generarse distintos tipos de pads (por ejemplo, el de audio y
	el de vídeo), no podrá pasarse como parámetro el elemento a conectar
	sino que tendrá que ser la propia función escuchadora la que 1)
	determine qué tipo de pad se ha creado y 2) realice las conexiones
	adecuadas en función de ello.  Por tanto, cambiará la función que maneja
	la conexión de pads creados dinámicamente (pad dinámicos del
	demultiplexor), así como la instrucción en que se asocia dicho manejador
	a la señal <em>PAD_ADDED</em>:
      </p>
<ul>
  <li>
    Respecto al registro del manejador, ahora no se sabe a priori a qué
    elemento habrá que conectar el demultiplexor (si a la cola de audio o
    a la de vídeo).  De modo que no puede pasarse como parámetro el
    elemento destino al registrarlo, sino que se pasa NULL.
  </li>
  <li>
    Respecto a la función, hay que comprobar el tipo de pad creado, por
    ejemplo comprobando sus capabilities.
    <ul>
      <li>La función <em>gst_pad_get_name(GstPad_REFERENCE)</em> devuelve
      el nombre del pad. </li>
      <li>La función <em>gst_pad_get_caps(GstPad_REFERENCE)</em> devuelve
      un objeto GstCaps con las capabilities del pad.</li>
      <li>La función <em>gst_caps_to_string(GstCaps_REFERENCE)</em>
      convierte un objeto GstCaps a una cadena de caracteres.</li>
    </ul>
    Por tanto, puedes recuperar las capabilities del pad y comprobar si
    corresponden a un flujo de audio o video. Consulta en
    el <a href="http://www.gtk.org/api/2.6/glib/index.html">manual de
    referencia de GLib</a> la documentación
    sobre <a href="http://www.gtk.org/api/2.6/glib/glib-String-Utility-Functions.html">funciones
    para manejar Strings</a>.
  </li>
      </ul>

    </section>



    <section id="bash">
      <h2>Comandos bash útiles</h2>
      <ul>
	<li>
	  <span class="code">man</span>: 
	  muestra la ayuda de un comando. Por ejemplo:
	  <pre class="code">man ls</pre>
	  Muestra la ayuda sobre el comando ls.
	  Para salir de la ayuda: pulsa la letra 'q'.
	  <br /><br />
	</li>
	<li>
	  <span class="code">ls</span>: lista los contenidos de un directorio
	  <br /><br />
	</li>
	<li>
	  <span class="code">cd</span>: (change directory) cambia de directorio
	  <ul>
	    <li><span class="code">.</span>: representa el directorio actual</li>
	    <li><span class="code">..</span>: representa el directorio padre</li>
	  </ul>
	  <br />
	</li>
	<li>
	  <span class="code">grep</span>: 
	  filtra las líneas que incluyan un determinado texto. Por ejemplo:
	  <pre class="code">grep palabra nombre_fichero</pre>
	  filtra el fichero <em>nombre_fichero</em>, presentando sólo las líneas que incluyen el texto <em>palabra</em>.
	  <p>
	    Combinado con los <em>pipelines</em> de linux (¡no
	    confundir con los <em>pipelines</em> de GStreamer, aunque
	    la idea es similar!), permite filtrar la salida estandar generada por un comando.
	    Ejemplo:
	  </p>
	  <pre class="code">gst-inspect | grep sink</pre>
	  filtra el resultado del comando gst-inspect, presentando sólo las líneas que incluyen el texto <em>sink</em>.
	  <br />
	</li>
      </ul>
    </section>
      
    <section id="refs">
      <a name="refs" />
      <h2>Referencias</h2>
      <ul>
        <li><a href="http://gstreamer.freedesktop.org/documentation/">Documentación de GStreamer</a>
	<ul>
	<li><a href="http://gstreamer.freedesktop.org/documentation/plugins.html">Overview of all Plug-ins</a></li>
        <li><a href="http://gstreamer.freedesktop.org/data/doc/gstreamer/head/manual/html/index.html">GStreamer Application Development Manual (0.10.36)</a></li>
	</ul>
        <li><a href="https://aulaglobal2.uc3m.es/file.php/29384/Tema_8/Tema8._GStreamer_I_.pdf">Apuntes de la asignatura</a></li>
        <li>Manual de bash</li>
	<li>Documentación sobre Ogg
	  <ul>
	<li><a href="http://es.wikipedia.org/wiki/Ogg">Información básica sobre el formato Ogg</a></li>
	<li><a href="http://www.xiph.org/ogg/doc/">Documentación oficial de Ogg</a></li>
	</ul>
      </ul>
    </section>
    
    <footer>
      <figure>
        <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/es/">
        <img alt="Licencia Creative Commons" style="border-width:0" src="http://i.creativecommons.org/l/by-sa/3.0/es/88x31.png" /></a>
        <figcaption>Este <span xmlns:dct="http://purl.org/dc/terms/" href="http://purl.org/dc/dcmitype/Text" rel="dct:type">obra</span> 
        está bajo una <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/es/">licencia Creative Commons 
        Reconocimiento-CompartirIgual 3.0 España</a>.</figcaption>
      </figure>
      <details>
        <summary>Recurso desarollado por Raquel M. Crespo (<a href="www.it.uc3m.es/rcrespo/">www.it.uc3m.es/rcrespo/</a>), data de <time datetime="2012-02-06">2012-02-06</time>.</summary>
      </details>
      <br /><br /><br />
    </footer>
    <script type="text/javascript" src="../js/socket.io.min.js"></script>
    <script type="text/javascript" src="../js/classon.js?session=p11"></script>
  </body>
</html>
